{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31c2829a-aec0-45c8-bb43-c15c6d231a06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This is done as a notebook which works best with DataBricks. Otherwise, it would be split in two python files: ``n_model.py`` with the model class definition, and ``train_nn_model``.py where the data is loaded and the model is trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "208d317c-cadc-411e-9899-0c4498a79c91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# nn_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0854aef4-d22d-4914-9a15-a592b61b1e36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import mlflow.pyfunc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Neural network class\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hidden_sizes=(32, 32), \n",
    "                 dropout=0.2,\n",
    "                 country_idx=0,\n",
    "                 year_idx=1,\n",
    "                 n_countries=None, \n",
    "                 embed_dim=4):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Columns where year and country are\n",
    "        self.year_idx, self.country_idx = year_idx, country_idx\n",
    "\n",
    "        # Embeddings for the countries\n",
    "        self.country_embed = nn.Embedding(n_countries, embed_dim)\n",
    "\n",
    "        # Layer setup\n",
    "        hidden_sizes = [embed_dim+1] + list(hidden_sizes)\n",
    "        layers = []\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], 1))\n",
    "        self.fc_layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            Predicts the TOE_HAB/MTOE given the country and year.\n",
    "            x: [country (encoded int), year (standarized float32)]\n",
    "            output: predicted standarized TOE_HAB/MTOE\n",
    "        \"\"\"\n",
    "        x = torch.cat([x[:,self.year_idx].reshape(-1,1), self.country_embed(x[:,self.country_idx].long())], dim=1)\n",
    "        return self.fc_layers(x)\n",
    "\n",
    "\n",
    "# Neural network model class\n",
    "class NNModel(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, \n",
    "                 hidden_sizes=(32, 32),\n",
    "                 lr=0.01,\n",
    "                 dropout=0.2,\n",
    "                 n_countries=37,\n",
    "                 embed_dim=4):\n",
    "        \n",
    "        # Custom neural network\n",
    "        self.net = Net(hidden_sizes=hidden_sizes,\n",
    "                         dropout=dropout,\n",
    "                         n_countries=n_countries,\n",
    "                         embed_dim=embed_dim)\n",
    "        \n",
    "        # Scalers for standarizing the input and output values\n",
    "        self.year_scaler, self.output_scaler = StandardScaler(), StandardScaler()\n",
    "        self.country_idx, self.year_idx = 0, 1\n",
    "\n",
    "        # Optimizer and loss function for training\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=lr)\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def _prepare_training_data(self, X, y):\n",
    "        \"\"\" Prepares the training data for the neural network.\n",
    "            X: pd.DataFrame[country (int), year (int)].\n",
    "            y: pd.DataFrame[TOE_HAB/MTOE (float)].\n",
    "            output: np.array(X), np.array(y)\n",
    "        \"\"\"\n",
    "        # Prepare data\n",
    "        assert type(X) is pd.DataFrame and type(y) is pd.DataFrame, \"Train data must be DataFrames\"\n",
    "        assert \"country_encoded\" in X.columns,  \"X DataFrame must contain country_encoded column\"\n",
    "        assert \"year\" in X.columns, \"X DataFrame must contain year column\"\n",
    "        self.year_scaler.fit(X[\"year\"].values.reshape(-1,1))\n",
    "        self.output_scaler.fit(y.values)\n",
    "        X[\"year\"] = self.year_scaler.transform(X[\"year\"].values.reshape(-1,1))\n",
    "        X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        y = torch.tensor(self.output_scaler.transform(y.values), dtype=torch.float32)\n",
    "        return X, y\n",
    "\n",
    "    def fit(self, X, y, epochs=100, test_frac=0, mlflow_run=False, verbose=False):\n",
    "        \"\"\" \n",
    "            Trains the neural network.\n",
    "            X: pd.DataFrame[country (int), year (int)].\n",
    "            y: pd.DataFrame[TOE_HAB/MTOE (float)].\n",
    "        \"\"\"\n",
    "        X, y = self._prepare_training_data(X.copy(), y.copy())\n",
    "        \n",
    "        if test_frac > 0:\n",
    "            n_test = int(test_frac*len(X))\n",
    "            X_train, X_test = X[:-n_test], X[-n_test:]\n",
    "            y_train, y_test = y[:-n_test], y[-n_test:]\n",
    "        else:\n",
    "            X_train, y_train =  X, y\n",
    "\n",
    "        # Train loop\n",
    "        for epoch in range(epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "            y_hat = self.net(X_train)\n",
    "            loss = self.mse(y_hat, y_train)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            train_loss = loss.item()\n",
    "\n",
    "            # If there is test data, compute test MSE\n",
    "            if test_frac > 0:\n",
    "                y_hat = self.net(X_test)\n",
    "                val_loss = self.mse(y_hat, y_test).item()\n",
    "                if mlflow_run:\n",
    "                    mlflow.log_metrics({\"train_mse\": train_loss, \"test_mse\": val_loss}, step=epoch+1)\n",
    "                if (epoch+1) % 10 == 0 and verbose:\n",
    "                    print(f\"Epoch {epoch+1}, Train loss: {train_loss:.4f}, Test loss: {val_loss:.4f}\")\n",
    "            else:\n",
    "                if mlflow_run:\n",
    "                    mlflow.log_metric(\"train_mse\", train_loss, step=epoch+1)\n",
    "                if (epoch+1) % 10 == 0 and verbose:\n",
    "                    print(f\"Epoch {epoch+1}, Train loss: {train_loss:.4f}\")\n",
    "\n",
    "        self.net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07437326-99f4-4979-b805-511bb9ce308e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# train_nn_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5ac6f5a-31ba-46a4-a084-d3317e47c4cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading table\n\nApplying feature engineering\n\nPreparing training data\nData shapes: X_train: (813, 2), y_train: (813, 1)\n\nTraining neural network\nEpoch 10, Train loss: 0.5638, Test loss: 0.3893\nEpoch 20, Train loss: 0.0880, Test loss: 0.0593\nEpoch 30, Train loss: 0.0272, Test loss: 0.0223\nEpoch 40, Train loss: 0.0112, Test loss: 0.0131\nEpoch 50, Train loss: 0.0071, Test loss: 0.0092\nEpoch 60, Train loss: 0.0055, Test loss: 0.0069\nEpoch 70, Train loss: 0.0045, Test loss: 0.0061\nEpoch 80, Train loss: 0.0040, Test loss: 0.0059\nEpoch 90, Train loss: 0.0037, Test loss: 0.0051\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 18:30:46 INFO mlflow.pyfunc: Inferring model signature from input example\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Train loss: 0.0034, Test loss: 0.0054\n\nSaving neural network model\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import mlflow.pyfunc\n",
    "import mlflow\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import joblib\n",
    "import random\n",
    "\n",
    "# Seed the libraries for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Load table\n",
    "print(\"Loading table\")\n",
    "df = spark.read.table(\"energy_clean\")\n",
    "df = df.toPandas()\n",
    "\n",
    "# Feature engineering\n",
    "print(\"\\nApplying feature engineering\")\n",
    "country_encoder = LabelEncoder()\n",
    "df.insert(1, \"country_encoded\", country_encoder.fit_transform(df[\"country\"]), allow_duplicates=True)\n",
    "\n",
    "# Prepare training data\n",
    "print(\"\\nPreparing training data\")\n",
    "df = sklearn.utils.shuffle(df)\n",
    "target_col = [\"TOE_HAB\"]\n",
    "feature_cols = [\"country_encoded\", \"year\"]\n",
    "X_train = df[feature_cols].astype(\"float32\")\n",
    "y_train = df[target_col].astype(\"float32\")\n",
    "n_countries = len(df[\"country\"].unique().tolist())\n",
    "print(f\"Data shapes: X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"train_nn\"):\n",
    "    print(\"\\nTraining neural network\")\n",
    "    \n",
    "    # Initialize model\n",
    "    params = {\n",
    "        \"hidden_sizes\": (32, 32), \n",
    "        \"lr\": 0.05, \n",
    "        \"dropout\": 0.0, \n",
    "        \"fit_epochs\": 100,\n",
    "        \"n_countries\": n_countries,\n",
    "        \"embed_dim\": 4\n",
    "    }\n",
    "    model = NNModel(hidden_sizes=params[\"hidden_sizes\"],\n",
    "                lr=params[\"lr\"],\n",
    "                dropout=params[\"dropout\"], \n",
    "                n_countries=n_countries,\n",
    "                embed_dim=params[\"embed_dim\"])\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train, epochs=params[\"fit_epochs\"], test_frac=0.05, mlflow_run=True, verbose=True)\n",
    "\n",
    "    # Log with MLFlow and save model\n",
    "    print(\"\\nSaving neural network model\")\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.pyfunc.log_model(\"neural_network\", \n",
    "                            python_model=model, \n",
    "                            input_example=X_train.iloc[0:1],\n",
    "                            )\n",
    "    joblib.dump(country_encoder, \"country_encoder.pkl\")\n",
    "    joblib.dump(model.year_scaler, \"year_scaler.pkl\")\n",
    "    joblib.dump(model.output_scaler, \"output_scaler.pkl\")\n",
    "    torch.save(model.net.state_dict(), \"neural_network.pt\")\n",
    "    joblib.dump(params, \"parameters.pkl\")\n",
    "    mlflow.pytorch.log_model(model.net,\n",
    "                             artifact_path=\"neural_network\", \n",
    "                             input_example=np.array([[21.0, 2005.0]]).astype(\"float32\"))\n",
    "    mlflow.log_artifact(\"country_encoder.pkl\")\n",
    "    mlflow.log_artifact(\"year_scaler.pkl\")\n",
    "    mlflow.log_artifact(\"output_scaler.pkl\")\n",
    "    mlflow.log_artifact(\"neural_network.pt\")\n",
    "    mlflow.log_artifact(\"parameters.pkl\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "/Workspace/Users/andrewkm210@gmail.com/environment_energy_ml_training.yaml",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "train_nn",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}