{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e647a39-36f3-4439-84f0-5cdbd946dac9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Load table to a pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a92b53e-a163-46e1-85f3-d09d6a4f6aaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "First, we load the table using pyspark into a pandas DataFrame. This makes feature engineering convenient by using the pandas and scikit-learn packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "006cb854-adef-44a8-88e7-77b749214038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>country</th><th>year</th><th>MTOE</th><th>TOE_HAB</th></tr></thead><tbody><tr><td>SK</td><td>2019</td><td>11.2</td><td>2.05</td></tr><tr><td>BA</td><td>2018</td><td>4.4</td><td>1.26</td></tr><tr><td>PL</td><td>2018</td><td>74.9</td><td>1.97</td></tr><tr><td>CZ</td><td>2011</td><td>24.5</td><td>2.33</td></tr><tr><td>LV</td><td>2007</td><td>4.4</td><td>1.98</td></tr><tr><td>DE</td><td>2021</td><td>208.1</td><td>2.5</td></tr><tr><td>IT</td><td>2015</td><td>116.2</td><td>1.91</td></tr><tr><td>LV</td><td>2019</td><td>4.1</td><td>2.13</td></tr><tr><td>TR</td><td>2001</td><td>50.7</td><td>0.78</td></tr><tr><td>FI</td><td>2022</td><td>23.3</td><td>4.2</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "SK",
         2019,
         11.2,
         2.05
        ],
        [
         "BA",
         2018,
         4.4,
         1.26
        ],
        [
         "PL",
         2018,
         74.9,
         1.97
        ],
        [
         "CZ",
         2011,
         24.5,
         2.33
        ],
        [
         "LV",
         2007,
         4.4,
         1.98
        ],
        [
         "DE",
         2021,
         208.1,
         2.5
        ],
        [
         "IT",
         2015,
         116.2,
         1.91
        ],
        [
         "LV",
         2019,
         4.1,
         2.13
        ],
        [
         "TR",
         2001,
         50.7,
         0.78
        ],
        [
         "FI",
         2022,
         23.3,
         4.2
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "year",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "MTOE",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "TOE_HAB",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data table\n",
    "df = spark.read.table(\"energy_clean\")\n",
    "df = df.toPandas()\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e2d81c6-26b8-486c-a5af-1c9185baf147",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Feature engineering and training data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81fc746d-f85e-4225-93c4-8904d29c69c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The data in the table has already been cleaned and preprocessed in the previous ``energy_preprocessing`` notebook. However, depending on the ML algorithm, some additional feature engineering may be needed. For example, most ML algorithms do not handle strings or categorical columns well, for example the country one. \n",
    "\n",
    "Instead of using strings, we can encode the column to integer values. Even if the values are now integers, the values are still categorical, which can be handled by neural networks but may not be optimal for tree based methods. This is because the trees work assuming a logical order between mumerical values, which does not make sense with categorical data (21 (Spain) > 20 (Germany)). For this, we can use one-hot encoding, which creates a column for each country with 0 (not this country) or 1 (yes). The drawback is that this can explode the size of the dataset, depending on the number of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9480fdb-03e7-4c91-a371-6366aaca2689",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1758628828137}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>country</th><th>country_encoded</th><th>year</th><th>MTOE</th><th>TOE_HAB</th><th>country_AT</th><th>country_BA</th><th>country_BE</th><th>country_BG</th><th>country_CY</th><th>country_CZ</th><th>country_DE</th><th>country_DK</th><th>country_EE</th><th>country_EL</th><th>country_ES</th><th>country_FI</th><th>country_FR</th><th>country_HR</th><th>country_HU</th><th>country_IE</th><th>country_IS</th><th>country_IT</th><th>country_LT</th><th>country_LU</th><th>country_LV</th><th>country_ME</th><th>country_MK</th><th>country_MT</th><th>country_NL</th><th>country_NO</th><th>country_PL</th><th>country_PT</th><th>country_RO</th><th>country_RS</th><th>country_SE</th><th>country_SI</th><th>country_SK</th><th>country_TR</th><th>country_UK</th><th>country_XK</th></tr></thead><tbody><tr><td>SK</td><td>33</td><td>2019</td><td>11.2</td><td>2.05</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td></tr><tr><td>BA</td><td>2</td><td>2018</td><td>4.4</td><td>1.26</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>PL</td><td>27</td><td>2018</td><td>74.9</td><td>1.97</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>CZ</td><td>6</td><td>2011</td><td>24.5</td><td>2.33</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>LV</td><td>21</td><td>2007</td><td>4.4</td><td>1.98</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>DE</td><td>7</td><td>2021</td><td>208.1</td><td>2.5</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>IT</td><td>18</td><td>2015</td><td>116.2</td><td>1.91</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>LV</td><td>21</td><td>2019</td><td>4.1</td><td>2.13</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>TR</td><td>34</td><td>2001</td><td>50.7</td><td>0.78</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td></tr><tr><td>FI</td><td>12</td><td>2022</td><td>23.3</td><td>4.2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "SK",
         33,
         2019,
         11.2,
         2.05,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         1,
         0,
         0,
         0
        ],
        [
         "BA",
         2,
         2018,
         4.4,
         1.26,
         0,
         1,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "PL",
         27,
         2018,
         74.9,
         1.97,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         1,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "CZ",
         6,
         2011,
         24.5,
         2.33,
         0,
         0,
         0,
         0,
         0,
         1,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "LV",
         21,
         2007,
         4.4,
         1.98,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         1,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "DE",
         7,
         2021,
         208.1,
         2.5,
         0,
         0,
         0,
         0,
         0,
         0,
         1,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "IT",
         18,
         2015,
         116.2,
         1.91,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         1,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "LV",
         21,
         2019,
         4.1,
         2.13,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         1,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0
        ],
        [
         "TR",
         34,
         2001,
         50.7,
         0.78,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         1,
         0,
         0
        ],
        [
         "FI",
         12,
         2022,
         23.3,
         4.2,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         1,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country_encoded",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "year",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "MTOE",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "TOE_HAB",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "country_AT",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_BA",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_BE",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_BG",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_CY",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_CZ",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_DE",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_DK",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_EE",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_EL",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_ES",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_FI",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_FR",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_HR",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_HU",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_IE",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_IS",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_IT",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_LT",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_LU",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_LV",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_ME",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_MK",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_MT",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_NL",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_NO",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_PL",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_PT",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_RO",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_RS",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_SE",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_SI",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_SK",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_TR",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_UK",
         "type": "\"byte\""
        },
        {
         "metadata": "{}",
         "name": "country_XK",
         "type": "\"byte\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# One hot encoding, should work best for Random Forests, but increases training data dimensionality\n",
    "df[\"country\"] = df[\"country\"].astype(\"category\")\n",
    "df[\"country_copy\"] = df[\"country\"]\n",
    "df = pd.get_dummies(df, \n",
    "                    columns=[\"country_copy\"], \n",
    "                    prefix=\"country\",\n",
    "                    drop_first=True, \n",
    "                    dtype=np.int8) # np.uint8 is not supported by pyspark\n",
    "\n",
    "# Simple encoding which can be used as a categorical feature for XGBoost, more memory efficient\n",
    "# Will also be used for embedding in the neural network\n",
    "country_encoder = LabelEncoder()\n",
    "df.insert(1, \"country_encoded\", country_encoder.fit_transform(df[\"country\"]), allow_duplicates=True)\n",
    "display(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04e04950-d441-4fb7-a655-6bc3388aba67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To demonstrate how well the ML algorithms work, we split the data into the train data used for tuning and training the ML models, and the test data which will only be used to compare which ML model works best. To showcase the model's capabilities of predicting future values, the test data is the last 4 years of data, and will not be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b7edab6-9124-4899-8b12-b09bb8507c13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# For demonstration purposes only, we will take the last 4 years of data for testing purposes\n",
    "# This allows us to show how well the models generalize temporally\n",
    "# The final model will be trained on a random 80/20 split as commonly done\n",
    "df = df.sort_values(by=\"year\", ascending=True)\n",
    "df_train = df[df[\"year\"] <= 2018].copy()\n",
    "df_test = df[df[\"year\"] > 2018].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a336ad94-f804-4080-83af-8e6907cfdb44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Machine Learning training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f225dde8-725f-4359-9006-85e6b5a03217",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e9c65aa-efd1-492a-b054-693c8a85187c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The first ML algorithm we are going to try is the Random Forest regressor. This works by creating an ensemble of decision trees. Each decision tree is trained on a subset of the data. The trees fit the data by learning simple decision rules. The first node of the tree could be ``x > 2015``, which splits the tree in two nodes based on the answer. The leaves contain the average value for the data that lead to that node. \n",
    "\n",
    "``n_estimators`` controls the number of trees and ``max_depth`` the maximum depth of the trees. To tune these parameters, as will be done with the other ML algorithms, we use Cross-validation. This method splits the training data into k splits (k=5 in this case). For each of the chosen parameter combinations, it will train on k-1 sets and test on the remaining set. The parameters that lead to the best overal root mean squared error (RMSE) will be selected. Given that the data is temporally correlated, we use ``TimeSeriesSplit`` which splits the training data based on time instead of randomly.\n",
    "\n",
    "To see the difference of using one-hot encoding for the countries compared to simple numerical encoding, we tune and train to separate models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feac597b-ae83-4f8d-a2d2-b5ce8d3ec90a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nFitting 5 folds for each of 12 candidates, totalling 60 fits\nBest Parameters: {'max_depth': 30, 'n_estimators': 200}\n\nTraining data shapes -> X_train: (681, 2), y_train: (681,), X_test: (132, 2), y_test: (132,)\n\nTraining random forest regressor\nRMSE: 0.294, R2: 0.956\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Test with simple country encoding\n",
    "target_col = \"TOE_HAB\"\n",
    "feature_cols = [\"country_encoded\", \"year\"]\n",
    "\n",
    "X, y = df_train[feature_cols], df_train[target_col]\n",
    "\n",
    "# Do a simple grid seach to find the best parameters\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 20, 30],\n",
    "}\n",
    "\n",
    "# Use time series splits for temporally correlated data\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "print()\n",
    "grid_search = GridSearchCV(RandomForestRegressor(), \n",
    "                           param_grid=param_grid, \n",
    "                           cv=tscv, \n",
    "                           scoring=\"neg_root_mean_squared_error\",\n",
    "                           verbose=True)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "max_depth = grid_search.best_params_[\"max_depth\"]\n",
    "n_estimators = grid_search.best_params_[\"n_estimators\"]\n",
    "\n",
    "# Prepare train and test data\n",
    "X_train, y_train = np.array(df_train[feature_cols]), np.array(df_train[target_col])\n",
    "X_test, y_test = np.array(df_test[feature_cols]), np.array(df_test[target_col])\n",
    "print(f\"\\nTraining data shapes -> X_train: {X_train.shape}, y_train: {y_train.shape}, X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "# Train RandomForestRegressor\n",
    "print(f\"\\nTraining random forest regressor\")\n",
    "with mlflow.start_run(run_name=\"random_forest_regressor_simple\"):\n",
    "    \n",
    "    # Initialize model\n",
    "    rf = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n",
    "    \n",
    "    # Train model\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and measure performance\n",
    "    y_pred = rf.predict(X_test)\n",
    "    rmse = root_mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Log with MLFlow\n",
    "    mlflow.log_param(\"n_estimators\", n_estimators)\n",
    "    mlflow.log_param(\"max_depth\", max_depth)\n",
    "    mlflow.log_param(\"random_state\", 42)\n",
    "    mlflow.log_param(\"country_encoding\", \"simple\")\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "\n",
    "    mlflow.sklearn.log_model(rf, \"random_forest_regressor_simple\", input_example=X_train[0:1])\n",
    "\n",
    "print(f\"RMSE: {rmse:.3f}, R2: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5c25547-edd6-4d19-bcd8-c6ad8a708aff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nFitting 5 folds for each of 12 candidates, totalling 60 fits\nBest Parameters: {'max_depth': 30, 'n_estimators': 100}\n\nTraining data shapes -> X_train: (681, 37), y_train: (681,), X_test: (132, 37), y_test: (132,)\n\nTraining random forest regressor\nRMSE: 0.317, R2: 0.949\n"
     ]
    }
   ],
   "source": [
    "# Test with one-hot country encoding, should work betteree\n",
    "target_col = \"TOE_HAB\"\n",
    "feature_cols = [\"year\"] + [col for col in df.columns if (col.startswith(\"country_\") and not col.endswith(\"encoded\"))]\n",
    "\n",
    "X, y = df_train[feature_cols], df_train[target_col]\n",
    "\n",
    "# Do a simple grid seach to find the best parameters\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 20, 30],\n",
    "}\n",
    "\n",
    "# Use time series splits for temporally correlated data\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "print()\n",
    "grid_search = GridSearchCV(RandomForestRegressor(), \n",
    "                           param_grid=param_grid, \n",
    "                           cv=tscv, \n",
    "                           scoring=\"neg_root_mean_squared_error\",\n",
    "                           verbose=True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "max_depth = grid_search.best_params_[\"max_depth\"]\n",
    "n_estimators = grid_search.best_params_[\"n_estimators\"]\n",
    "\n",
    "# Prepare test and train data\n",
    "X_train, y_train = np.array(df_train[feature_cols]), np.array(df_train[target_col])\n",
    "X_test, y_test = np.array(df_test[feature_cols]), np.array(df_test[target_col])\n",
    "print(f\"\\nTraining data shapes -> X_train: {X_train.shape}, y_train: {y_train.shape}, X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "# Train RandomForestRegressor\n",
    "print(f\"\\nTraining random forest regressor\")\n",
    "with mlflow.start_run(run_name=\"random_forest_regressor_one_hot\"):\n",
    "\n",
    "    # Initialize model\n",
    "    rf = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n",
    "    \n",
    "    # Train model\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and measure performance\n",
    "    y_pred = rf.predict(X_test)\n",
    "    rmse = root_mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Log with MLFlow\n",
    "    mlflow.log_param(\"n_estimators\", n_estimators)\n",
    "    mlflow.log_param(\"max_depth\", max_depth)\n",
    "    mlflow.log_param(\"random_state\", 42)\n",
    "    mlflow.log_param(\"country_encoding\", \"one_hot\")\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "\n",
    "    mlflow.sklearn.log_model(rf, \"random_forest_regressor_one_hot\", input_example=X_train[0:1])\n",
    "\n",
    "print(f\"RMSE: {rmse:.3f}, R2: {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b823c1e-0f87-4b58-83c9-63f0972b8c9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Even though one-hot encoding for the countries should represent better the categorical values, the trees seem to work better using numerical encoding. This can happen, one-hot encoding with many labels (37 in this case) leads to many more features which can complicate training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1371cef9-ab37-477f-8c99-05cdbd73c761",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4ad4fe1-6735-4782-8da4-891e1430d935",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "XGBoost is also based on decision trees. Instead of creating N trees that fit on subsets of data, the ``n_estimators`` trees are created sequentally. When a new tree is created, it tries to correct the errors of the previous trees by minimizing the error using gradient descent. The ``learning_rate`` parameter controls the influence of previous trees on the current tree. The ``subsample`` parameter controls show much data each tree uses for fitting. \n",
    "\n",
    "Again, we use cross-validation to find the best parameters. Given the compute limitation, we do a randomized search which tests only 50 parameter combinations. The xgboost library now handles categorical data (which applies one-hot encoding internally), so we don't need to do separate tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b18e25df-b09f-4f33-bb77-eacd75e62412",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nFitting 5 folds for each of 50 candidates, totalling 250 fits\nBest Parameters: {'subsample': 1, 'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.3}\n\nTraining data shapes -> X_train: (681, 2), y_train: (681,), X_test: (132, 2), y_test: (132,)\n\nTraining XGBoost\n[0]\tvalidation_0-rmse:1.26746\tvalidation_1-rmse:1.02916\n[1]\tvalidation_0-rmse:0.97125\tvalidation_1-rmse:0.75436\n[2]\tvalidation_0-rmse:0.76919\tvalidation_1-rmse:0.60292\n[3]\tvalidation_0-rmse:0.59383\tvalidation_1-rmse:0.46718\n[4]\tvalidation_0-rmse:0.49999\tvalidation_1-rmse:0.40849\n[5]\tvalidation_0-rmse:0.40367\tvalidation_1-rmse:0.34629\n[6]\tvalidation_0-rmse:0.33433\tvalidation_1-rmse:0.30144\n[7]\tvalidation_0-rmse:0.28770\tvalidation_1-rmse:0.28991\n[8]\tvalidation_0-rmse:0.24619\tvalidation_1-rmse:0.26525\n[9]\tvalidation_0-rmse:0.21208\tvalidation_1-rmse:0.25950\n[10]\tvalidation_0-rmse:0.19201\tvalidation_1-rmse:0.25288\n[11]\tvalidation_0-rmse:0.16423\tvalidation_1-rmse:0.25013\n[12]\tvalidation_0-rmse:0.14712\tvalidation_1-rmse:0.24242\n[13]\tvalidation_0-rmse:0.13299\tvalidation_1-rmse:0.24285\n[14]\tvalidation_0-rmse:0.12451\tvalidation_1-rmse:0.25031\n[15]\tvalidation_0-rmse:0.11802\tvalidation_1-rmse:0.24872\n[16]\tvalidation_0-rmse:0.10606\tvalidation_1-rmse:0.25858\n[17]\tvalidation_0-rmse:0.09959\tvalidation_1-rmse:0.26481\nRMSE: 0.242, R2: 0.970\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import mlflow\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "\n",
    "# This version of XGBoost handles categorical data (with non-strings only), so we don't need one-hot encoding\n",
    "target_col = \"TOE_HAB\"\n",
    "feature_cols = [\"country_encoded\", \"year\"]\n",
    "\n",
    "# This version of XGBoost does not like integers\n",
    "for df_i in [df_train, df_test]:\n",
    "    df_i[\"country_encoded\"] = df_i[\"country_encoded\"].astype(\"double\")\n",
    "    df_i[\"year\"] = df_i[\"year\"].astype(\"double\")\n",
    "\n",
    "X, y = df_train[feature_cols], df_train[target_col]\n",
    "\n",
    "# Do a simple random seach to find the best parameters, instead of grid search because of limited resources\n",
    "params_search = {\n",
    "    'n_estimators': [50, 100, 300],\n",
    "    'max_depth': [3, 6, 9], # smaller trees work better in XGBoost\n",
    "    \"learning_rate\": [0.05, 0.1, 0.3],\n",
    "    \"subsample\": [0.5, 0.75, 1]\n",
    "}\n",
    "\n",
    "# Use time series splits for temporally correlated data\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "print()\n",
    "param_search = RandomizedSearchCV(xgb.XGBRegressor(enable_categorical=True), \n",
    "                                  params_search, \n",
    "                                  n_iter=50, \n",
    "                                  cv=tscv, \n",
    "                                  scoring=\"neg_root_mean_squared_error\", \n",
    "                                  verbose=True)\n",
    "param_search.fit(X_train, y_train)\n",
    "print(\"Best Parameters:\", param_search.best_params_)\n",
    "max_depth = param_search.best_params_[\"max_depth\"]\n",
    "n_estimators = param_search.best_params_[\"n_estimators\"]\n",
    "learning_rate = param_search.best_params_[\"learning_rate\"]\n",
    "subsample = param_search.best_params_[\"subsample\"]\n",
    "\n",
    "# Prepare train and test data\n",
    "X_train, y_train = df_train[feature_cols], df_train[target_col]\n",
    "X_test, y_test = df_test[feature_cols], df_test[target_col]\n",
    "print(f\"\\nTraining data shapes -> X_train: {X_train.shape}, y_train: {y_train.shape}, X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "# Train XGBoostRegressor\n",
    "print(f\"\\nTraining XGBoost\")\n",
    "with mlflow.start_run(run_name=\"xgboost_regressor\"):\n",
    "\n",
    "    # Define model\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        learning_rate=learning_rate,\n",
    "        subsample=subsample,\n",
    "        enable_categorical=True,\n",
    "        eval_metric=\"rmse\",\n",
    "        early_stopping_rounds=5, # Stop if the validation RMSE does not improve in 5 steps\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    eval_set = [(X_train, y_train), (X_test,y_test)]\n",
    "    xgb_model.fit(X_train, y_train, eval_set=eval_set)\n",
    "\n",
    "    # Predict and measure performance\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "    results = xgb_model.evals_result()\n",
    "    rmse = root_mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Log with MLflow\n",
    "    for i, (train_rmse, val_rmse) in enumerate(zip(results[\"validation_0\"][\"rmse\"], results[\"validation_1\"][\"rmse\"])):\n",
    "        mlflow.log_metric(\"train_rmse\", train_rmse, step=i)\n",
    "        mlflow.log_metric(\"val_rmse\", val_rmse, step=i)\n",
    "    mlflow.log_param(\"n_estimators\", n_estimators)\n",
    "    mlflow.log_param(\"max_depth\", max_depth)\n",
    "    mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "    mlflow.log_param(\"subsample\", subsample)\n",
    "    mlflow.log_param(\"random_state\", 42)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "\n",
    "    mlflow.xgboost.log_model(xgb_model, artifact_path=\"xgboost_regressor\", input_example=X_train[0:1], model_format='ubj')\n",
    "\n",
    "    print(f\"RMSE: {rmse:.3f}, R2: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aad201d7-e962-4dd3-a2d0-c8a4364429d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ae1ca78-8b79-48e2-95b0-3db0990951ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Neural networks are composed of interconnected artificial neurons. An activation function associated to a weight is usually used to represent the artificial neuron. The neural network is typically divided into layers, with each layer having a specific number of neurons (the size). The size of the input/output layer is the number of input/output features. The rest of the layers are called the hidden layers, which can have any number of neurons. The input value to the neural network is propagated through the neurons of the layers, leading to an output on the final layer. To train the neural network, the weights are adjusted based on the prediction error using algorithms like gradient descent.\n",
    "\n",
    "In this case, we create our own custom PyTorch neural network with a custom number of layers and sizes. To deal with the country categorical input we can use embeddings, which maps the categorical value to a vector of numerical values. These output values are also learned while training the neural network. We can also introduce an optional dropout layer between the hidden layers, which will randomly deactivate neurons to prevent overfitting. We can tune the parameters implementing a custom Cross-validation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44630dc0-a108-481e-9d4f-de5b8e539649",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes:\n X_train: (681, 2)\n y_train: (681, 1)\n X_test: (132, 2)\n y_test: (132, 1)\n\nTuning parameters\nConfigurations tried: 0/50 - Best RMSE: 0.4834\nConfigurations tried: 10/50 - Best RMSE: 0.3562\nConfigurations tried: 20/50 - Best RMSE: 0.3410\nConfigurations tried: 30/50 - Best RMSE: 0.3021\nConfigurations tried: 40/50 - Best RMSE: 0.3021\nBest RMSE: 0.2942, at index 49\nBest params:\n {'dropout': 0, 'fit_epochs': 100, 'hidden_sizes': (32, 32), 'lr': 0.05}\n\nTraining neural network\nEpoch 10, Train loss: 0.5750, Test loss: 0.5398\nEpoch 20, Train loss: 0.1453, Test loss: 0.1317\nEpoch 30, Train loss: 0.0603, Test loss: 0.0422\nEpoch 40, Train loss: 0.0289, Test loss: 0.0178\nEpoch 50, Train loss: 0.0087, Test loss: 0.0252\nEpoch 60, Train loss: 0.0059, Test loss: 0.0210\nEpoch 70, Train loss: 0.0047, Test loss: 0.0173\nEpoch 80, Train loss: 0.0043, Test loss: 0.0159\nEpoch 90, Train loss: 0.0040, Test loss: 0.0171\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/30 18:22:43 INFO mlflow.pyfunc: Inferring model signature from input example\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Train loss: 0.0038, Test loss: 0.0179\nRMSE: 0.20443028264007856\n\nSaving neural network model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import joblib\n",
    "import mlflow.pyfunc\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Seed the libraries for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# Neural network class\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hidden_sizes=(32, 32), \n",
    "                 dropout=0.2,\n",
    "                 country_idx=0,\n",
    "                 year_idx=1,\n",
    "                 n_countries=None, \n",
    "                 embed_dim=4):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Columns where year and country are\n",
    "        self.year_idx, self.country_idx = year_idx, country_idx\n",
    "\n",
    "        # Embeddings for the countries\n",
    "        self.country_embed = nn.Embedding(n_countries, embed_dim)\n",
    "\n",
    "        # Layer setup\n",
    "        hidden_sizes = [embed_dim+1] + list(hidden_sizes)\n",
    "        layers = []\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            layers.append(nn.ReLU())\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "        layers.append(nn.Linear(hidden_sizes[-1], 1))\n",
    "        self.fc_layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            Predicts the TOE_HAB/MTOE given the country and year.\n",
    "            x: [country (encoded int), year (standarized float32)]\n",
    "            output: predicted standarized TOE_HAB/MTOE\n",
    "        \"\"\"\n",
    "        x = torch.cat([x[:,self.year_idx].reshape(-1,1), self.country_embed(x[:,self.country_idx].long())], dim=1)\n",
    "        return self.fc_layers(x)\n",
    "\n",
    "\n",
    "# Neural network model class\n",
    "class NNModel(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, \n",
    "                 hidden_sizes=(32, 32),\n",
    "                 lr=0.01,\n",
    "                 dropout=0.2,\n",
    "                 n_countries=37,\n",
    "                 embed_dim=4):\n",
    "        \n",
    "        # Custom neural network\n",
    "        self.net = Net(hidden_sizes=hidden_sizes,\n",
    "                         dropout=dropout,\n",
    "                         n_countries=n_countries,\n",
    "                         embed_dim=embed_dim)\n",
    "        \n",
    "        # Scalers for standarizing the input and output values\n",
    "        self.year_scaler, self.output_scaler = StandardScaler(), StandardScaler()\n",
    "        self.country_idx, self.year_idx = 0, 1\n",
    "\n",
    "        # Optimizer and loss function for training\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=lr)\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def _prepare_training_data(self, X, y):\n",
    "        \"\"\" Prepares the training data for the neural network.\n",
    "            X: pd.DataFrame[country (int), year (int)].\n",
    "            y: pd.DataFrame[TOE_HAB/MTOE (float)].\n",
    "            output: np.array(X), np.array(y)\n",
    "        \"\"\"\n",
    "        # Prepare data\n",
    "        assert type(X) is pd.DataFrame and type(y) is pd.DataFrame, \"Train data must be DataFrames\"\n",
    "        assert \"country_encoded\" in X.columns,  \"X DataFrame must contain country_encoded column\"\n",
    "        assert \"year\" in X.columns, \"X DataFrame must contain year column\"\n",
    "        self.year_scaler.fit(X[\"year\"].values.reshape(-1,1))\n",
    "        self.output_scaler.fit(y.values)\n",
    "        X[\"year\"] = self.year_scaler.transform(X[\"year\"].values.reshape(-1,1))\n",
    "        X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        y = torch.tensor(self.output_scaler.transform(y.values), dtype=torch.float32)\n",
    "        return X, y\n",
    "\n",
    "    def fit(self, X, y, epochs=100, test_frac=0, mlflow_run=False, verbose=False):\n",
    "        \"\"\" \n",
    "            Trains the neural network.\n",
    "            X: pd.DataFrame[country (int), year (int)].\n",
    "            y: pd.DataFrame[TOE_HAB/MTOE (float)].\n",
    "        \"\"\"\n",
    "        X, y = self._prepare_training_data(X.copy(), y.copy())\n",
    "        \n",
    "        if test_frac > 0:\n",
    "            n_test = int(test_frac*len(X))\n",
    "            X_train, X_test = X[:-n_test], X[-n_test:]\n",
    "            y_train, y_test = y[:-n_test], y[-n_test:]\n",
    "        else:\n",
    "            X_train, y_train =  X, y\n",
    "\n",
    "        # Train loop\n",
    "        for epoch in range(epochs):\n",
    "            self.optimizer.zero_grad()\n",
    "            y_hat = self.net(X_train)\n",
    "            loss = self.mse(y_hat, y_train)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            train_loss = loss.item()\n",
    "\n",
    "            # If there is test data, compute test MSE\n",
    "            if test_frac > 0:\n",
    "                y_hat = self.net(X_test)\n",
    "                val_loss = self.mse(y_hat, y_test).item()\n",
    "                if mlflow_run:\n",
    "                    mlflow.log_metrics({\"train_mse\": train_loss, \"test_mse\": val_loss}, step=epoch+1)\n",
    "                if (epoch+1) % 10 == 0 and verbose:\n",
    "                    print(f\"Epoch {epoch+1}, Train loss: {train_loss:.4f}, Test loss: {val_loss:.4f}\")\n",
    "            else:\n",
    "                if mlflow_run:\n",
    "                    mlflow.log_metric(\"train_mse\", train_loss, step=epoch+1)\n",
    "                if (epoch+1) % 10 == 0 and verbose:\n",
    "                    print(f\"Epoch {epoch+1}, Train loss: {train_loss:.4f}\")\n",
    "\n",
    "        self.net.eval()\n",
    "\n",
    "    def predict(self, model_input: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"\n",
    "            Returns the predicted TOE_HAB/MTOE for the given input.\n",
    "            model_input: pd.DataFrame[country (encoded int), year (int)].\n",
    "            output: predicted pd.Series[TOE_HAB/MTOE (float)].\n",
    "        \"\"\"\n",
    "        model_input = model_input.copy()\n",
    "        model_input[\"year\"] = self.year_scaler.transform(model_input[\"year\"].values.reshape(-1,1))\n",
    "        model_input = torch.tensor(model_input.values, dtype=torch.float32)\n",
    "        with torch.no_grad():\n",
    "            y_hat = self.net(model_input)\n",
    "        y_hat = self.output_scaler.inverse_transform(y_hat)\n",
    "        return pd.Series(y_hat.reshape(-1))\n",
    "\n",
    "    def evaluate_rmse(self, X,  y):\n",
    "        \"\"\"\n",
    "            Returns the RMSE of the model on the given data.\n",
    "            X: [country (encoded int), year (int)]. \n",
    "            y: [TOE_HAB/MTOE].\n",
    "            output: RMSE.\n",
    "        \"\"\"\n",
    "        assert type(X) is pd.DataFrame and type(y) is pd.DataFrame, \"Data must be DataFrames\"\n",
    "        assert \"country_encoded\" in X.columns,  \"X DataFrame must contain country_encoded column\"\n",
    "        assert \"year\" in X.columns, \"X DataFrame must contain year column\"\n",
    "        with torch.no_grad():\n",
    "            y_pred = self.predict(X.copy())\n",
    "        return np.sqrt(np.mean((y_pred.values - y.values.reshape(-1))**2))\n",
    "        \n",
    "\n",
    "# Neural networks also benefit from one-hot encoding, however we can use an embedding layer for memory efficiency\n",
    "target_col = [\"TOE_HAB\"]\n",
    "feature_cols = [\"country_encoded\", \"year\"]\n",
    "\n",
    "# Prepare train and test data\n",
    "X_train = df_train[feature_cols].astype(\"float32\")\n",
    "X_test = df_test[feature_cols].astype(\"float32\")\n",
    "y_train = df_train[target_col].astype(\"float32\")\n",
    "y_test = df_test[target_col].astype(\"float32\")\n",
    "n_countries = len(df[\"country\"].unique().tolist())\n",
    "print(f\"Data shapes:\\n X_train: {X_train.shape}\\n y_train: {y_train.shape}\\n X_test: {X_test.shape}\\n y_test: {y_test.shape}\")\n",
    "\n",
    "# Tune parameters using cross-validation with a randomized search\n",
    "print(\"\\nTuning parameters\")\n",
    "from sklearn.model_selection import TimeSeriesSplit, ParameterGrid\n",
    "param_grid = {\n",
    "    \"hidden_sizes\": [(16,), (32,), (16, 16), (32, 32)],\n",
    "    \"lr\": [0.005, 0.01, 0.05],\n",
    "    \"dropout\": [0, 0.1, 0.2],\n",
    "    \"fit_epochs\": [50, 100, 150, 200]}\n",
    "param_grid = list(ParameterGrid(param_grid))\n",
    "param_grid = np.random.choice(param_grid, 50, replace=False)\n",
    "tscv = TimeSeriesSplit(n_splits=5) # Time splits\n",
    "\n",
    "# Cross-validation\n",
    "best_rmse = np.inf\n",
    "best_idx = 0\n",
    "for i, params in enumerate(param_grid):\n",
    "    rmses = []\n",
    "    for tr_index, val_index in tscv.split(X_train):\n",
    "        X_tr, X_val = X_train.iloc[tr_index], X_train.iloc[val_index]\n",
    "        y_tr, y_val = y_train.iloc[tr_index], y_train.iloc[val_index]\n",
    "        model = NNModel(hidden_sizes=params[\"hidden_sizes\"],\n",
    "                        lr=params[\"lr\"],\n",
    "                        dropout=params[\"dropout\"], \n",
    "                        n_countries=n_countries,\n",
    "                        embed_dim=4)\n",
    "        model.fit(X_tr, y_tr, epochs=params[\"fit_epochs\"], test_frac=0, mlflow_run=False, verbose=False)\n",
    "        rmses.append(model.evaluate_rmse(X_val, y_val))\n",
    "    rmse = np.mean(rmses)\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_idx = i\n",
    "    if i%10 == 0:\n",
    "        print(f\"Configurations tried: {i}/{len(param_grid)} - Best RMSE: {best_rmse:.4f}\")\n",
    "    rmses = []\n",
    "print(f\"Best RMSE: {best_rmse:.4f}, at index {best_idx}\")\n",
    "params = param_grid[best_idx]\n",
    "print(\"Best params:\\n\", params)\n",
    "\n",
    "print(\"\\nTraining neural network\")\n",
    "with mlflow.start_run(run_name=\"neural_network\"):\n",
    "\n",
    "    # Initialize neural network model with best parameters found\n",
    "    model = NNModel(hidden_sizes=params[\"hidden_sizes\"],\n",
    "                lr=params[\"lr\"],\n",
    "                dropout=params[\"dropout\"], \n",
    "                n_countries=n_countries,\n",
    "                embed_dim=4)\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train, epochs=params[\"fit_epochs\"], test_frac=0.05, mlflow_run=True, verbose=True)\n",
    "\n",
    "    # Evaluate model on test data\n",
    "    rmse = model.evaluate_rmse(X_test, y_test)\n",
    "    print(\"RMSE:\", rmse)\n",
    "\n",
    "    # Log with MLFlow\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    print(\"\\nSaving neural network model\")\n",
    "    mlflow.pyfunc.log_model(\"neural_network\", \n",
    "                            python_model=model, \n",
    "                            input_example=X_train.iloc[0:1],\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcbab3f2-b9ac-4ad4-b8a3-ed51483e53b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Predict with trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4efd49eb-fedb-4709-9df3-4867ca78a1f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The two best models are the XGBoost and neural network ones, with a RMSE of ~0.24 and ~0.20 respectively. In this case, we will choose to continue with the neural network model, which achieved a slightly improved RMSE. The final model will be trained in the ``energy_nn_training`` notebook. The following cell shows the predicted TOE for Spain for the years 2000-2022 compared to the ground truth. Unlike with XGBoost, the predicted values of the neural network form a smoother trend curve, and the model does not overfit to the training data. Unfortunately, it cannot predict outliers like in 2020 due to covid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9739a89b-7b0a-42c4-aba4-a502867f6864",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>country</th><th>country_encoded</th><th>year</th><th>TOE_HAB</th><th>xgb_predicted_TOE_HAB</th><th>nn_predicted_TOE_HAB</th></tr></thead><tbody><tr><td>ES</td><td>11</td><td>2000</td><td>1.97</td><td>2.0543656</td><td>2.0321082651449087</td></tr><tr><td>ES</td><td>11</td><td>2001</td><td>2.06</td><td>2.0543656</td><td>2.053221288144897</td></tr><tr><td>ES</td><td>11</td><td>2002</td><td>2.06</td><td>2.0543656</td><td>2.0737591265160837</td></tr><tr><td>ES</td><td>11</td><td>2003</td><td>2.15</td><td>2.1226888</td><td>2.092308136187962</td></tr><tr><td>ES</td><td>11</td><td>2004</td><td>2.21</td><td>2.1226888</td><td>2.100087834319733</td></tr><tr><td>ES</td><td>11</td><td>2005</td><td>2.25</td><td>2.1226888</td><td>2.1059940710855933</td></tr><tr><td>ES</td><td>11</td><td>2006</td><td>2.16</td><td>2.1226888</td><td>2.0982163253625887</td></tr><tr><td>ES</td><td>11</td><td>2007</td><td>2.18</td><td>2.1226888</td><td>2.074491830482935</td></tr><tr><td>ES</td><td>11</td><td>2008</td><td>2.07</td><td>2.1226888</td><td>2.041703352997227</td></tr><tr><td>ES</td><td>11</td><td>2009</td><td>1.9</td><td>1.9708691</td><td>2.001392870306051</td></tr><tr><td>ES</td><td>11</td><td>2010</td><td>1.92</td><td>1.8809162</td><td>1.9615794008003689</td></tr><tr><td>ES</td><td>11</td><td>2011</td><td>1.86</td><td>1.8481071</td><td>1.9212803321912124</td></tr><tr><td>ES</td><td>11</td><td>2012</td><td>1.79</td><td>1.8481071</td><td>1.8995846152979363</td></tr><tr><td>ES</td><td>11</td><td>2013</td><td>1.74</td><td>1.8481071</td><td>1.8801914391433425</td></tr><tr><td>ES</td><td>11</td><td>2014</td><td>1.71</td><td>1.8481071</td><td>1.8736029106062184</td></tr><tr><td>ES</td><td>11</td><td>2015</td><td>1.73</td><td>1.8481071</td><td>1.8670175359601786</td></tr><tr><td>ES</td><td>11</td><td>2016</td><td>1.77</td><td>1.8481071</td><td>1.8601557703192646</td></tr><tr><td>ES</td><td>11</td><td>2017</td><td>1.82</td><td>1.8843325</td><td>1.8539643817499616</td></tr><tr><td>ES</td><td>11</td><td>2018</td><td>1.85</td><td>1.8843325</td><td>1.8475885656448647</td></tr><tr><td>ES</td><td>11</td><td>2019</td><td>1.83</td><td>1.8843325</td><td>1.8404146148483558</td></tr><tr><td>ES</td><td>11</td><td>2020</td><td>1.56</td><td>1.8843325</td><td>1.8345010690650616</td></tr><tr><td>ES</td><td>11</td><td>2021</td><td>1.69</td><td>1.8843325</td><td>1.8276178769894786</td></tr><tr><td>ES</td><td>11</td><td>2022</td><td>1.7</td><td>1.8843325</td><td>1.8212174805586288</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "ES",
         11,
         2000,
         1.97,
         2.0543656,
         2.0321082651449087
        ],
        [
         "ES",
         11,
         2001,
         2.06,
         2.0543656,
         2.053221288144897
        ],
        [
         "ES",
         11,
         2002,
         2.06,
         2.0543656,
         2.0737591265160837
        ],
        [
         "ES",
         11,
         2003,
         2.15,
         2.1226888,
         2.092308136187962
        ],
        [
         "ES",
         11,
         2004,
         2.21,
         2.1226888,
         2.100087834319733
        ],
        [
         "ES",
         11,
         2005,
         2.25,
         2.1226888,
         2.1059940710855933
        ],
        [
         "ES",
         11,
         2006,
         2.16,
         2.1226888,
         2.0982163253625887
        ],
        [
         "ES",
         11,
         2007,
         2.18,
         2.1226888,
         2.074491830482935
        ],
        [
         "ES",
         11,
         2008,
         2.07,
         2.1226888,
         2.041703352997227
        ],
        [
         "ES",
         11,
         2009,
         1.9,
         1.9708691,
         2.001392870306051
        ],
        [
         "ES",
         11,
         2010,
         1.92,
         1.8809162,
         1.9615794008003689
        ],
        [
         "ES",
         11,
         2011,
         1.86,
         1.8481071,
         1.9212803321912124
        ],
        [
         "ES",
         11,
         2012,
         1.79,
         1.8481071,
         1.8995846152979363
        ],
        [
         "ES",
         11,
         2013,
         1.74,
         1.8481071,
         1.8801914391433425
        ],
        [
         "ES",
         11,
         2014,
         1.71,
         1.8481071,
         1.8736029106062184
        ],
        [
         "ES",
         11,
         2015,
         1.73,
         1.8481071,
         1.8670175359601786
        ],
        [
         "ES",
         11,
         2016,
         1.77,
         1.8481071,
         1.8601557703192646
        ],
        [
         "ES",
         11,
         2017,
         1.82,
         1.8843325,
         1.8539643817499616
        ],
        [
         "ES",
         11,
         2018,
         1.85,
         1.8843325,
         1.8475885656448647
        ],
        [
         "ES",
         11,
         2019,
         1.83,
         1.8843325,
         1.8404146148483558
        ],
        [
         "ES",
         11,
         2020,
         1.56,
         1.8843325,
         1.8345010690650616
        ],
        [
         "ES",
         11,
         2021,
         1.69,
         1.8843325,
         1.8276178769894786
        ],
        [
         "ES",
         11,
         2022,
         1.7,
         1.8843325,
         1.8212174805586288
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "country",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "country_encoded",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "year",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "TOE_HAB",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "xgb_predicted_TOE_HAB",
         "type": "\"float\""
        },
        {
         "metadata": "{}",
         "name": "nn_predicted_TOE_HAB",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyBQcmVwYXJlIHZhbGlkYXRpb24gZGF0YSwgaW4gdGhpcyBjYXNlIGRhdGEgZm9yIFNwYWluCmRmX3ZhbCA9IGRmW1siY291bnRyeSIsICJjb3VudHJ5X2VuY29kZWQiLCAieWVhciIsICJUT0VfSEFCIl1dLmNvcHkoKQpkZl92YWwgPSBkZl92YWxbZGZfdmFsWyJjb3VudHJ5Il0gPT0gIkVTIl0KWF92YWwgPSBkZl92YWxbWyJjb3VudHJ5X2VuY29kZWQiLCAieWVhciJdXQoKIyBQcmVkaWN0IHdpdGggWEdCb29zdCBhbmQgbmV1cmFsIG5ldHdvcmsKeV9wcmVkX3hnYiA9IHhnYl9tb2RlbC5wcmVkaWN0KFhfdmFsKQp3aXRoIHRvcmNoLm5vX2dyYWQoKToKICAgIHlfcHJlZF9ubiA9IG1vZGVsLnByZWRpY3QoWF92YWwpCgojIFZpc3VhbGl6ZSBwcmVkaWN0aW9ucwpkZl92YWxbInhnYl9wcmVkaWN0ZWRfVE9FX0hBQiJdID0geV9wcmVkX3hnYi5yZXNoYXBlKC0xLDEpCmRmX3ZhbFsibm5fcHJlZGljdGVkX1RPRV9IQUIiXSA9IHlfcHJlZF9ubi52YWx1ZXMKZGlzcGxheShkZl92YWwp\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewa94c326\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewa94c326\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewa94c326\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewa94c326) SELECT `year`,SUM(`TOE_HAB`) `column_de2103dc87`,SUM(`xgb_predicted_TOE_HAB`) `column_de2103dc90`,SUM(`nn_predicted_TOE_HAB`) `column_de2103dc97` FROM q GROUP BY `year`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewa94c326\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Anual TOE_HAB prediction for Spain",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "year",
             "id": "column_de2103dc85"
            },
            "y": [
             {
              "column": "TOE_HAB",
              "id": "column_de2103dc87",
              "transform": "SUM"
             },
             {
              "column": "xgb_predicted_TOE_HAB",
              "id": "column_de2103dc90",
              "transform": "SUM"
             },
             {
              "column": "nn_predicted_TOE_HAB",
              "id": "column_de2103dc97",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "line",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_de2103dc87": {
             "name": "Real",
             "type": "line",
             "yAxis": 0
            },
            "column_de2103dc90": {
             "name": "Predicted by XGBoost",
             "type": "line",
             "yAxis": 0
            },
            "column_de2103dc97": {
             "name": "Predicted by Neural Network",
             "type": "line",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "title": {
             "text": "Year"
            },
            "type": "-"
           },
           "yAxis": [
            {
             "title": {
              "text": "TOE_HAB"
             },
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "84f3bc92-e257-4017-8754-13f2013ae81a",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 8.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "finished",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "year",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "year",
           "type": "column"
          },
          {
           "alias": "column_de2103dc87",
           "args": [
            {
             "column": "TOE_HAB",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          },
          {
           "alias": "column_de2103dc90",
           "args": [
            {
             "column": "xgb_predicted_TOE_HAB",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          },
          {
           "alias": "column_de2103dc97",
           "args": [
            {
             "column": "nn_predicted_TOE_HAB",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": [],
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare validation data, in this case data for Spain\n",
    "df_val = df[[\"country\", \"country_encoded\", \"year\", \"TOE_HAB\"]].copy()\n",
    "df_val = df_val[df_val[\"country\"] == \"ES\"]\n",
    "X_val = df_val[[\"country_encoded\", \"year\"]]\n",
    "\n",
    "# Predict with XGBoost and neural network\n",
    "y_pred_xgb = xgb_model.predict(X_val)\n",
    "with torch.no_grad():\n",
    "    y_pred_nn = model.predict(X_val)\n",
    "\n",
    "# Visualize predictions\n",
    "df_val[\"xgb_predicted_TOE_HAB\"] = y_pred_xgb.reshape(-1,1)\n",
    "df_val[\"nn_predicted_TOE_HAB\"] = y_pred_nn.values\n",
    "display(df_val)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "xgboost",
     "torch",
     "threadpoolctl==3.1.0"
    ],
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "energy_ml_training",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}